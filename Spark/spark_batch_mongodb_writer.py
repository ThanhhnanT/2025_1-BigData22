"""
PySpark Batch Job: Äá»c tá»« Redis hoáº·c Kafka vÃ  lÆ°u vÃ o MongoDB
- CÃ³ thá»ƒ cháº¡y tá»« Airflow
- Äá»c dá»¯ liá»‡u ngÃ y hÃ´m qua
- LÆ°u vÃ o MongoDB vá»›i batch processing
"""
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, from_json, struct, to_json
from pyspark.sql import functions as F
from pyspark.sql.types import (
    StructType, StringType, DoubleType, BooleanType, LongType, TimestampType
)
from datetime import datetime, timedelta, timezone
import os
import sys

# Config
KAFKA_BROKER = os.getenv("KAFKA_BROKER", "192.168.49.2:30113")
KAFKA_TOPIC = os.getenv("KAFKA_TOPIC", "crypto_kline_1m")
MONGO_URI = os.getenv("MONGO_URI", "mongodb://localhost:27017")
MONGO_DB = os.getenv("MONGO_DB", "crypto_history")
MONGO_COLLECTION = os.getenv("MONGO_COLLECTION", "candles")
REDIS_HOST = os.getenv("REDIS_HOST", "localhost")
REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))

# TÃ­nh thá»i gian: hÃ´m qua
vn_tz = timezone(timedelta(hours=7))
now = datetime.now(vn_tz)
yesterday_start = (now - timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
yesterday_end = yesterday_start + timedelta(days=1) - timedelta(seconds=1)

start_timestamp = int(yesterday_start.timestamp() * 1000)
end_timestamp = int(yesterday_end.timestamp() * 1000)

print(f"ðŸ“… Xá»­ lÃ½ dá»¯ liá»‡u tá»« {yesterday_start} Ä‘áº¿n {yesterday_end}")
print(f"   Timestamp: {start_timestamp} - {end_timestamp}")

# Spark Session
spark = SparkSession.builder \
    .appName("CryptoBatchMongoWriter") \
    .config("spark.jars.packages", 
        "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,"
        "org.mongodb.spark:mongo-spark-connector_2.12:3.0.1") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

# Option 1: Äá»c tá»« Kafka (náº¿u cÃ³ dá»¯ liá»‡u trong retention period)
try:
    print("\nðŸ“– Äang Ä‘á»c tá»« Kafka...")
    
    kafka_df = spark.read \
        .format("kafka") \
        .option("kafka.bootstrap.servers", KAFKA_BROKER) \
        .option("subscribe", KAFKA_TOPIC) \
        .option("startingOffsets", f"{{'{KAFKA_TOPIC}': {{'0': {start_timestamp}}}}}") \
        .option("endingOffsets", f"{{'{KAFKA_TOPIC}': {{'0': {end_timestamp}}}}") \
        .load()
    
    # Schema
    kline_schema = StructType() \
        .add("t", LongType(), True) \
        .add("T", LongType(), True) \
        .add("s", StringType(), True) \
        .add("i", StringType(), True) \
        .add("o", StringType(), True) \
        .add("c", StringType(), True) \
        .add("h", StringType(), True) \
        .add("l", StringType(), True) \
        .add("v", StringType(), True) \
        .add("q", StringType(), True) \
        .add("n", LongType(), True) \
        .add("x", BooleanType(), True)
    
    # Parse vÃ  transform
    kline_df = kafka_df \
        .selectExpr("CAST(value AS STRING) as json_value") \
        .select(from_json(col("json_value"), kline_schema).alias("data")) \
        .select(
            col("data.t").alias("openTime"),
            col("data.T").alias("closeTime"),
            col("data.s").alias("symbol"),
            col("data.i").alias("interval"),
            col("data.o").cast(DoubleType()).alias("open"),
            col("data.c").cast(DoubleType()).alias("close"),
            col("data.h").cast(DoubleType()).alias("high"),
            col("data.l").cast(DoubleType()).alias("low"),
            col("data.v").cast(DoubleType()).alias("volume"),
            col("data.q").cast(DoubleType()).alias("quoteVolume"),
            col("data.n").alias("trades"),
            col("data.x").alias("is_closed")
        ) \
        .filter(
            (col("openTime") >= start_timestamp) & 
            (col("openTime") <= end_timestamp) &
            (col("is_closed") == True)
        )
    
    # LÆ°u vÃ o MongoDB
    mongo_df = kline_df \
        .withColumn("createdAt", F.current_timestamp()) \
        .withColumn("source", lit("spark_batch_kafka"))
    
    print(f"âœ… ÄÃ£ Ä‘á»c {mongo_df.count()} records tá»« Kafka")
    
    # Ghi vÃ o MongoDB
    mongo_df.write \
        .format("mongo") \
        .mode("append") \
        .option("uri", MONGO_URI) \
        .option("database", MONGO_DB) \
        .option("collection", MONGO_COLLECTION) \
        .save()
    
    print(f"âœ… ÄÃ£ lÆ°u vÃ o MongoDB: {MONGO_DB}.{MONGO_COLLECTION}")
    
except Exception as e:
    print(f"âš ï¸  KhÃ´ng thá»ƒ Ä‘á»c tá»« Kafka: {e}")
    print("   CÃ³ thá»ƒ dá»¯ liá»‡u Ä‘Ã£ quÃ¡ retention period hoáº·c Kafka khÃ´ng cÃ³ dá»¯ liá»‡u")
    print("   Sá»­ dá»¥ng Redis hoáº·c source khÃ¡c...")

# Option 2: Äá»c tá»« Redis (náº¿u Kafka khÃ´ng cÃ³ dá»¯ liá»‡u)
# Note: PySpark khÃ´ng cÃ³ connector trá»±c tiáº¿p cho Redis
# CÃ³ thá»ƒ dÃ¹ng Python script Ä‘á»ƒ Ä‘á»c tá»« Redis vÃ  táº¡o DataFrame

spark.stop()
print("\nâœ… HoÃ n thÃ nh batch job!")

